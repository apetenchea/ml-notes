{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment classification task using BOW\n",
    "\n",
    "Inspired from fast.ai https://course18.fast.ai/ml\n",
    "\n",
    "A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Reviews with a score ranging from 5 to 6 are considered netural and thus are not included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "The aclmdb folder contains two folders: train and test. Each of these folders contains the neg and pos folders with movie reviews.\n",
    "A negative review will have be labeled with 0, while a positive review with 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_folder(name):\n",
    "    data = []\n",
    "    for verdict in ('neg', 'pos'):\n",
    "        for file in glob(os.path.join(name, verdict, '*.txt')):\n",
    "            data.append({\n",
    "                'text': open(file, encoding='utf8').read(),\n",
    "                'verdict': verdict == 'pos'\n",
    "            })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = parse_folder('../input/aclimdb/aclImdb/train/')\n",
    "df_test = parse_folder('../input/aclimdb/aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I rented this movie tonight because it looked like a fun movie. I figured that you really couldn't go wrong with a concept of Ex Girlfriend with super powers. <br /><br />... but the movie was confused and pointless ...<br /><br />it seemed that at every turn the writer kept throwing junk in. Also the writer kept throwing in way too much toilet humor and sexual situations that only a teenage boy could love.<br /><br />It seems that it could have been so simple to draw a story out of Fatal Attraction Super hero .. but I guess not. <br /><br />This is not a fun romantic comedy it was advertised to be. You could not take a child to see it and you would be embarrassed seeing it a date. <br /><br />If the writer could have done a basic story around the high concept and cleaned it up - the movie might have a fighting chance. <br /><br />A serious waste of time.<br /><br />B\",\n",
       " False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].text, df_train.iloc[0].verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW approach\n",
    "\n",
    "A Bag Of Words approach is a way to represent text in a manner that makes it usable in machine learning, which is usually a tensor (i.e a vector or an array). It is fairly easy to implement and can be quite effective, especially when dealing with short text messages, such as movie reviews.  \n",
    "For illustration, suppose there are only three reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['This movie is good', 'The movie is bad', 'Bad this movie was']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to define the *vocabulary*, which is the set of all words. Then the *document-term matrix* is created, which can be seen as a matrix interpretation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The', 'good', 'bad', 'Bad', 'This', 'is', 'this', 'was', 'movie'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for review in reviews:\n",
    "    for word in review.split(' '):\n",
    "        vocabulary.add(word)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The *document-term matrix* or *term-document matrix* is obtained by viewing every word in the vocabulary as a column, indicating its presence or number of occurences in every document. In this context a document is a movie review. This matrix is the *bag of words* representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!document</th>\n",
       "      <th>Bad</th>\n",
       "      <th>The</th>\n",
       "      <th>This</th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie is bad</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bad this movie was</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            !document  Bad  The  This  bad  good  is  movie  this  was\n",
       "0  This movie is good    0    0     1    0     1   1      1     0    0\n",
       "1    The movie is bad    0    1     0    1     0   1      1     0    0\n",
       "2  Bad this movie was    1    0     0    0     0   0      1     1    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_term = []\n",
    "for document in reviews:\n",
    "    row = {'!document': document}\n",
    "    row.update({word: document.split(' ').count(word) for word in vocabulary})\n",
    "    doc_term.append(row)\n",
    "doc_term = pd.DataFrame(doc_term)\n",
    "display(doc_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOW representation works on the principle that if the word *good* appears a lot in a movie review, it is very probable to be a positive one. This approach unfortunately doesn't take into account the order in which the words appear in a sentence, which is  fundamental to its meaning.  \n",
    "**The movie is shit!** and **The movie is the shit!** look very similar when considering their BOW representation, but these two sencences have very different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization refers to the process of turning a piece of text into a list of tokens or symbols, dealing with punctuation. For example, the text **You call this a \"movie\"?! It isn't good at all!** could be tokenized like this: **You call this a \" movie \" ?! It is n't good at all !**. The text is first separated into sentences, then each sentence is separated into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to do tokenization is with regular expressions, but [nltk](https://www.nltk.org/api/nltk.tokenize.html) contains lots of tokenizers to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’\\'“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize_re(text):\n",
    "    return re_tok.sub(r' \\1 ', text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nltk(text):\n",
    "    return list(itertools.chain.from_iterable(word_tokenize(sentence) for sentence in sent_tokenize(text)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Words such as gaming, gamed, games are replaced with game. Only the stems are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenize_nltk(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is used to create the matrix of token counts. Instead of actual words, there is the posibility to use n-grams. An n-gram is a tuple of n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=tokenize, max_features=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x1000000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12632304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_term = vectorizer.fit_transform(df_train.text)\n",
    "test_doc_term = vectorizer.transform(df_test.text)\n",
    "train_doc_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of columns are going to be 0, because only a small percentage of the vocabulary appears in each document. In order to keep it from exploding in memory, 0s are not stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary can be seen with *get_feature_names*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Using the term-document matrix it is possbile to infer the probability of a review being positive or negative.\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic probability rules\n",
    "\n",
    "Probability of C occuring, given D: $P(C|D)=\\frac{P(C \\wedge D)}{P(D)}$  \n",
    "Probability of both C and D occuring: $P(C \\wedge D)=P(C|D)*P(D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://ibin.co/4hxqDwhJnxCE.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://ibin.co/4hxqDwhJnxCE.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering probabilities using the term-document matrix\n",
    "Probability of class C being 1 (positive review), given a document D:\n",
    "$P(C=1|D)=\\frac{P(D \\wedge C=1)}{P(D)}=\\frac{P(D|C=1)*P(C=1)}{P(D)}$  \n",
    "By computing $\\frac{P(C=1|D)}{P(C=0|D)}$ we obtain $\\frac{P(D|C=1)}{P(D|C=0)}*\\frac{P(C=1)}{P(C=0)}$. If this number is greater than 1, the probability of the review being a positive one is greater than the probability of it being negative.  \n",
    "For each term we compute the probabilities of it appearing in a negative and in a positivie review.\n",
    "\n",
    "Learn more about probabilistic inference from [here](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/) (lectures 21 and 22)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.33333333])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0952381 , 0.0952381 , 0.0952381 , 0.0952381 , 0.0952381 ,\n",
       "        0.14285714, 0.19047619, 0.0952381 , 0.0952381 ],\n",
       "       [0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111,\n",
       "        0.11111111, 0.11111111, 0.11111111, 0.11111111]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = np.array([1, 0, 0])\n",
    "doc_term_mat = doc_term.drop('!document', axis='columns').values\n",
    "display(doc_term_mat)\n",
    "p_c = np.array([(classes == 0).mean(), (classes == 1).mean()])\n",
    "p_dc = np.ones((2, doc_term_mat.shape[1])) # use ones because by default every term can appear once in every class\n",
    "for col in range(doc_term_mat.shape[1]):\n",
    "    for row in range(doc_term_mat.shape[0]):\n",
    "        p_dc[classes[doc_term_mat[row][col]]][col] += doc_term_mat[row][col]\n",
    "for c in (0, 1):\n",
    "    p_dc[c] = p_dc[c] / p_dc[c].sum()\n",
    "display(p_c, p_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the probability of a document, given the class, we can multiply the probabilities of all its terms given the same class with the probability of that class occuring. For example, given the negative class, the probability of the first review belonging to it is approximately $0.095*0.095*0.142*0.190 * 0.66$. The last 0.66 comes from the 0.66 probability of the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it naive?\n",
    "\n",
    "It assumes that events are independent. In reality, the probability of the term *awful* appearing in a negative review is not independent of the probability of *bad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(train_doc_term, df_train.verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87084"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_doc_term, df_test.verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How similar it is to Logistic Regression?\n",
    "\n",
    "In order to convert multiplication into addition we can work with logarithms.  \n",
    "  \n",
    "$\\log(\\frac{P(D|C=1)}{P(D|C=0)}*\\frac{P(C=1)}{P(C=0)})=\\frac{P(D|C=1)}{P(D|C=0)}+\\log(\\frac{P(C=1)}{P(C=0)})$.  \n",
    "  \n",
    "Y = predicted values  \n",
    "  \n",
    "X = term-doc matrix  \n",
    "  \n",
    "W = $\\frac{P(D|C=1)}{P(D|C=0)}$  \n",
    "  \n",
    "B = $\\log(\\frac{P(C=1)}{P(C=0)})$  \n",
    "  \n",
    "$Y = X * W + B$\n",
    "\n",
    "Instead of learning these coefficients, we approximated them using a theoretical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15415068,  0.15415068,  0.15415068,  0.15415068,  0.15415068,\n",
       "       -0.25131443, -0.5389965 ,  0.15415068,  0.15415068])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-1.17515675, -1.17515675, -0.76969164])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = np.log(p_dc[1] / p_dc[0])\n",
    "bias = np.log(p_c[1] / p_c[0])\n",
    "display(weights, bias, doc_term_mat @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold is now 0 instead of 1, because $\\log 1 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_term_bool = train_doc_term > 0\n",
    "r_neg = (train_doc_term_bool[df_train.verdict.values == 0].sum(0) + 1) / (sum(df_train.verdict == 0) + 1)\n",
    "r_pos = (train_doc_term_bool[df_train.verdict.values == 1].sum(0) + 1) / (sum(df_train.verdict == 1) + 1)\n",
    "coef = np.log((r_pos / r_neg).A.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "By doing logistic regression, we try to fit an n-dimensional plane that separates positive reviews from negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90092"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(C=0.2, solver='liblinear', max_iter=500, dual=True) # C comes from regularization\n",
    "lreg.fit(train_doc_term, df_train.verdict)\n",
    "lreg.score(test_doc_term, df_test.verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the coefficients seems to yield better results than the theoretical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the two approaches\n",
    "\n",
    "We can initialize the coefficients with these obtained from Naive Bayes and start optimizing from there, instead of randomly initializing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle=False):\n",
    "    number_of_batches = ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "net = Sequential([\n",
    "    Dense(1, activation='sigmoid', input_dim=train_doc_term.shape[1], kernel_regularizer=l2(0.1)),\n",
    "])\n",
    "net.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    ModelCheckpoint('nn_best.h5', monitor='val_acc', verbose=0, save_weights_only=False, save_best_only=True, mode='max'),\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_weights([coef.reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train_doc_term, df_train.verdict.values, test_size=0.2, stratify=df_train.verdict.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 41s 517ms/step - loss: 21743.3950 - acc: 0.9834\n",
      "313/313 [==============================] - 205s 655ms/step - loss: 43490.7205 - acc: 0.9826 - val_loss: 21743.3950 - val_acc: 0.9834\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 41s 518ms/step - loss: 6209.3644 - acc: 0.9842\n",
      "313/313 [==============================] - 205s 656ms/step - loss: 12419.8577 - acc: 0.9844 - val_loss: 6209.3644 - val_acc: 0.9842\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 41s 517ms/step - loss: 1773.2425 - acc: 0.9848\n",
      "313/313 [==============================] - 206s 657ms/step - loss: 3546.8003 - acc: 0.9860 - val_loss: 1773.2425 - val_acc: 0.9848\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 41s 515ms/step - loss: 506.4203 - acc: 0.9884\n",
      "313/313 [==============================] - 206s 659ms/step - loss: 1012.8918 - acc: 0.9871 - val_loss: 506.4203 - val_acc: 0.9884\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 41s 516ms/step - loss: 144.6972 - acc: 0.9910\n",
      "313/313 [==============================] - 205s 656ms/step - loss: 289.3037 - acc: 0.9896 - val_loss: 144.6972 - val_acc: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0226be3da0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "net.fit_generator(\n",
    "    generator=batch_generator(X_train, Y_train, batch_size, shuffle=True),\n",
    "    validation_data=batch_generator(X_valid, Y_valid, batch_size), validation_steps=ceil(len(Y_valid) / batch_size),\n",
    "    epochs=5, steps_per_epoch=ceil(len(Y_train) / batch_size), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = load_model('nn_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144.91532070618456, 0.90224]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate_generator(batch_generator(test_doc_term, df_test.verdict, batch_size=batch_size), steps=ceil(len(df_test.verdict) / batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
